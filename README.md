# BERT study notes

Presentation at 2019-05-17 bioinformatics study group.

![](https://cdn-images-1.medium.com/fit/t/800/240/1*vb9tSSWGbutwC8E3fGaxfg.jpeg)

This is my humble attempt at studying Google's 
[BERT](https://github.com/google-research/bert)
([Devlin et al., 2018](https://arxiv.org/abs/1810.04805)) framework for NLP tasks.

I still prefer pytorch over tensorflow so most of the coding is done using packages built from pytorch.

## Useful resources

- https://medium.com/dissecting-bert
- https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/
- https://github.com/brightmart/bert_language_understanding
- (chinese) https://www.jiqizhixin.com/articles/2019-02-18-12
- (chinese) https://www.jiqizhixin.com/articles/2018-12-10-17
- (chinese) https://www.jiqizhixin.com/articles/2018-12-17-17
- https://github.com/huggingface/pytorch-pretrained-BERT
- https://github.com/codertimo/BERT-pytorch
- https://github.com/dhlee347/pytorchic-bert
